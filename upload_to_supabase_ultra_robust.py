import pandas as pd
from supabase import create_client, Client
import time
import os
import sys
import zipfile
import requests
from io import BytesIO
import urllib3
import ssl
from urllib.request import urlopen, Request
import subprocess
import json
import numpy as np
from datetime import datetime

print("üå≥ IBAMA Upload AUTO-FIX - Corre√ß√£o Autom√°tica de Schema v3.0...")

# --- 1. Configura√ß√£o ---
def get_env_var(key: str, default: str = None) -> str:
    value = os.getenv(key, default)
    if not value:
        raise ValueError(f"Vari√°vel de ambiente {key} n√£o encontrada!")
    return value

SUPABASE_URL = get_env_var("SUPABASE_URL")
SUPABASE_KEY = get_env_var("SUPABASE_KEY")
IBAMA_ZIP_URL = get_env_var(
    "IBAMA_ZIP_URL", 
    "https://dadosabertos.ibama.gov.br/dados/SIFISC/auto_infracao/auto_infracao/auto_infracao_csv.zip"
)

print(f"üìã Configura√ß√µes:")
print(f"  - Supabase URL: {SUPABASE_URL[:50]}...")

# --- 2. DETEC√á√ÉO AUTOM√ÅTICA DE SCHEMA ---
def get_existing_supabase_columns(supabase_client):
    """Obt√©m colunas existentes no Supabase atrav√©s de uma query SQL."""
    try:
        print("üîç Verificando estrutura atual da tabela...")
        
        # Usa query SQL para obter informa√ß√µes das colunas
        query = """
        SELECT column_name, data_type, is_nullable
        FROM information_schema.columns 
        WHERE table_name = 'ibama_infracao' 
        AND table_schema = 'public'
        ORDER BY ordinal_position;
        """
        
        result = supabase_client.rpc('exec_sql', {'query': query}).execute()
        
        if result.data:
            columns = [row['column_name'] for row in result.data]
            print(f"‚úÖ Encontradas {len(columns)} colunas na tabela")
            return columns
        else:
            print("‚ö†Ô∏è Nenhuma coluna encontrada - tabela pode n√£o existir")
            return []
            
    except Exception as e:
        print(f"‚ùå Erro ao consultar schema: {e}")
        
        # Fallback: tenta buscar um registro
        try:
            result = supabase_client.table('ibama_infracao').select('*').limit(1).execute()
            if result.data:
                columns = list(result.data[0].keys())
                print(f"‚úÖ Fallback: {len(columns)} colunas encontradas")
                return columns
            else:
                print("‚ö†Ô∏è Tabela existe mas est√° vazia")
                return []
        except Exception as e2:
            print(f"‚ùå Fallback tamb√©m falhou: {e2}")
            return []

def create_missing_columns_sql(supabase_client, missing_columns, sample_data):
    """Cria colunas faltantes usando SQL."""
    print(f"üîß Criando {len(missing_columns)} colunas faltantes...")
    
    created_columns = []
    failed_columns = []
    
    for col_name in missing_columns:
        try:
            # Detecta tipo baseado nos dados de amostra
            sample_value = sample_data.get(col_name)
            
            if pd.isna(sample_value) or sample_value is None:
                sql_type = "TEXT"
            elif isinstance(sample_value, (int, np.integer)):
                sql_type = "BIGINT"
            elif isinstance(sample_value, (float, np.floating)):
                sql_type = "DOUBLE PRECISION"  
            elif isinstance(sample_value, bool):
                sql_type = "BOOLEAN"
            else:
                sql_type = "TEXT"
            
            # SQL para criar coluna
            alter_sql = f'ALTER TABLE public.ibama_infracao ADD COLUMN IF NOT EXISTS "{col_name}" {sql_type};'
            
            print(f"  üìù Criando: {col_name} ({sql_type})")
            
            # Executa SQL
            result = supabase_client.rpc('exec_sql', {'query': alter_sql}).execute()
            created_columns.append(col_name)
            
        except Exception as e:
            print(f"  ‚ùå Falha ao criar {col_name}: {e}")
            failed_columns.append((col_name, str(e)))
    
    print(f"‚úÖ Colunas criadas: {len(created_columns)}")
    print(f"‚ùå Colunas falharam: {len(failed_columns)}")
    
    return created_columns, failed_columns

def auto_fix_schema(supabase_client, df_sample):
    """Corrige automaticamente o schema da tabela."""
    print("\nüîß AUTO-FIX: Corrigindo schema automaticamente...")
    
    # 1. Obt√©m colunas existentes
    existing_columns = get_existing_supabase_columns(supabase_client)
    csv_columns = list(df_sample.columns)
    
    print(f"üìä CSV: {len(csv_columns)} colunas")
    print(f"üìä Supabase: {len(existing_columns)} colunas")
    
    # 2. Identifica colunas faltantes
    existing_set = set(existing_columns)
    csv_set = set(csv_columns)
    
    missing_columns = csv_set - existing_set
    
    if missing_columns:
        print(f"üÜï Colunas faltantes: {len(missing_columns)}")
        for i, col in enumerate(list(missing_columns)[:10], 1):
            print(f"   {i:2d}. {col}")
        if len(missing_columns) > 10:
            print(f"   ... e mais {len(missing_columns) - 10} colunas")
        
        # 3. Cria colunas faltantes
        sample_record = df_sample.iloc[0].to_dict()
        created, failed = create_missing_columns_sql(supabase_client, missing_columns, sample_record)
        
        if failed:
            print("‚ö†Ô∏è Algumas colunas n√£o puderam ser criadas - continuando com as dispon√≠veis")
        
        # 4. Atualiza lista de colunas existentes
        updated_columns = get_existing_supabase_columns(supabase_client)
        return updated_columns
    
    else:
        print("‚úÖ Schema j√° est√° compat√≠vel")
        return existing_columns

def create_supabase_function_if_needed(supabase_client):
    """Cria fun√ß√£o SQL personalizada se n√£o existir."""
    try:
        # SQL para criar fun√ß√£o de execu√ß√£o se n√£o existir
        function_sql = """
        CREATE OR REPLACE FUNCTION public.exec_sql(query text)
        RETURNS json
        LANGUAGE plpgsql
        SECURITY DEFINER
        AS $$
        DECLARE
            result json;
        BEGIN
            EXECUTE query;
            GET DIAGNOSTICS result = ROW_COUNT;
            RETURN json_build_object('success', true, 'rows_affected', result);
        EXCEPTION
            WHEN OTHERS THEN
                RETURN json_build_object('success', false, 'error', SQLERRM);
        END;
        $$;
        """
        
        print("üîß Configurando fun√ß√£o SQL auxiliar...")
        supabase_client.rpc('exec_sql', {'query': function_sql}).execute()
        print("‚úÖ Fun√ß√£o SQL configurada")
        
    except Exception as e:
        print(f"‚ö†Ô∏è N√£o foi poss√≠vel criar fun√ß√£o SQL: {e}")

# --- 3. Download robusto (reutilizado) ---
def download_with_multiple_methods(url):
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    methods = [
        ("requests_no_ssl", lambda: download_with_requests_no_ssl(url)),
        ("urllib_no_ssl", lambda: download_with_urllib_no_ssl(url)),
        ("wget", lambda: download_with_wget(url)),
        ("curl", lambda: download_with_curl(url)),
    ]
    
    for method_name, method_func in methods:
        print(f"üîÑ Tentando m√©todo: {method_name}")
        try:
            content = method_func()
            if content and len(content) > 1000:
                print(f"‚úÖ Sucesso com {method_name}! Tamanho: {len(content):,} bytes")
                return content
        except Exception as e:
            print(f"‚ùå {method_name} falhou: {str(e)[:50]}...")
    
    raise Exception("‚ùå Todos os m√©todos de download falharam!")

def download_with_requests_no_ssl(url):
    session = requests.Session()
    session.verify = False
    response = session.get(url, timeout=300, 
                          headers={'User-Agent': 'Mozilla/5.0 (compatible; IBAMA-Bot/1.0)'})
    response.raise_for_status()
    return response.content

def download_with_urllib_no_ssl(url):
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    
    request = Request(url, headers={'User-Agent': 'Mozilla/5.0 (compatible; IBAMA-Bot/1.0)'})
    with urlopen(request, timeout=300, context=ssl_context) as response:
        return response.read()

def download_with_wget(url):
    try:
        result = subprocess.run([
            'wget', '--no-check-certificate', '--timeout=300',
            '--user-agent=Mozilla/5.0 (compatible; IBAMA-Bot/1.0)',
            '-O', '-', url
        ], capture_output=True, check=True, timeout=320)
        return result.stdout
    except:
        raise Exception("wget n√£o dispon√≠vel ou falhou")

def download_with_curl(url):
    try:
        result = subprocess.run([
            'curl', '-k', '--max-time', '300',
            '--user-agent', 'Mozilla/5.0 (compatible; IBAMA-Bot/1.0)',
            '-L', url
        ], capture_output=True, check=True, timeout=320)
        return result.stdout
    except:
        raise Exception("curl n√£o dispon√≠vel ou falhou")

# --- 4. Processamento de dados ---
def make_json_serializable(obj):
    if pd.isna(obj):
        return None
    elif isinstance(obj, (pd.Timestamp, datetime)):
        return obj.strftime('%Y-%m-%d %H:%M:%S') if pd.notna(obj) else None
    elif isinstance(obj, (np.integer, np.int64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64)):
        return float(obj) if not np.isnan(obj) else None
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, bytes):
        return obj.decode('utf-8', errors='ignore')
    else:
        return str(obj) if obj is not None else None

def clean_dataframe_for_json(df):
    print("üßπ Preparando dados para JSON...")
    df_clean = df.copy()
    
    for col in df_clean.columns:
        df_clean[col] = df_clean[col].apply(make_json_serializable)
    
    print(f"‚úÖ Dados limpos: {len(df_clean)} registros, {len(df_clean.columns)} colunas")
    return df_clean

def read_csv_robust(zip_file, csv_file):
    encodings = ['utf-8', 'latin1', 'cp1252']
    separators = [';', ',', '\t']
    
    for encoding in encodings:
        for sep in separators:
            try:
                with zip_file.open(csv_file) as csv_data:
                    df = pd.read_csv(csv_data, encoding=encoding, sep=sep, low_memory=False)
                    if len(df.columns) > 5 and len(df) > 0:
                        return df
            except:
                continue
    return None

def download_and_process_data():
    print("üì• Baixando dados do IBAMA...")
    
    content = download_with_multiple_methods(IBAMA_ZIP_URL)
    
    print("üì¶ Processando arquivo ZIP...")
    with zipfile.ZipFile(BytesIO(content)) as zip_file:
        csv_files = [f for f in zip_file.namelist() if f.endswith('.csv')]
        
        # Busca arquivos 2024-2025
        target_files = [f for f in csv_files if any(year in f for year in ['2024', '2025'])]
        
        if target_files:
            print(f"üéØ Arquivos encontrados: {target_files}")
        else:
            print("‚ö†Ô∏è Usando arquivos mais recentes...")
            target_files = sorted(csv_files, reverse=True)[:2]
        
        # Processa arquivos
        all_dataframes = []
        for csv_file in target_files:
            print(f"‚öôÔ∏è Processando: {csv_file}")
            df_temp = read_csv_robust(zip_file, csv_file)
            
            if df_temp is not None and len(df_temp) > 0:
                print(f"    ‚úÖ {len(df_temp):,} registros")
                all_dataframes.append(df_temp)
        
        if not all_dataframes:
            raise ValueError("Nenhum arquivo v√°lido processado")
        
        # Combina DataFrames
        df = pd.concat(all_dataframes, ignore_index=True, sort=False)
        print(f"üìä Dados combinados: {len(df):,} registros")
        
    return df

# --- 5. Upload AUTO-FIX ---
def upload_with_auto_fix(df):
    """Upload com corre√ß√£o autom√°tica de schema."""
    print("üîó Conectando ao Supabase...")
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    table_name = "ibama_infracao"
    
    # 1. Configurar fun√ß√£o SQL se necess√°rio
    create_supabase_function_if_needed(supabase)
    
    # 2. Auto-corrigir schema
    compatible_columns = auto_fix_schema(supabase, df.head(1))
    
    # 3. Filtrar DataFrame para colunas compat√≠veis
    available_columns = [col for col in df.columns if col in compatible_columns]
    df_filtered = df[available_columns].copy()
    
    print(f"‚úÖ DataFrame filtrado: {len(df_filtered)} registros, {len(available_columns)} colunas")
    
    if len(available_columns) < len(df.columns):
        skipped = len(df.columns) - len(available_columns)
        print(f"‚ö†Ô∏è {skipped} colunas foram puladas (n√£o compat√≠veis)")
    
    # 4. Limpeza de dados
    df_clean = clean_dataframe_for_json(df_filtered)
    
    # 5. Limpar tabela
    print(f"üßπ Limpando tabela...")
    try:
        supabase.table(table_name).delete().neq('id', -1).execute()
        print("‚úÖ Tabela limpa")
    except Exception as e:
        print(f"‚ö†Ô∏è Aviso ao limpar: {e}")
    
    # 6. Upload em lotes pequenos
    chunk_size = 200
    total_chunks = (len(df_clean) // chunk_size) + 1
    print(f"üöÄ Upload: {len(df_clean):,} registros em {total_chunks} lotes de {chunk_size}")
    
    successful_uploads = 0
    failed_uploads = 0
    
    for i in range(0, len(df_clean), chunk_size):
        chunk_index = i // chunk_size + 1
        print(f"  üì§ Lote {chunk_index}/{total_chunks}...", end=" ")
        
        chunk = df_clean[i:i + chunk_size]
        
        try:
            data_to_insert = chunk.to_dict(orient='records')
            
            # Upload com retry
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    response = supabase.table(table_name).insert(data_to_insert).execute()
                    
                    if hasattr(response, 'error') and response.error:
                        raise Exception(f"API Error: {response.error}")
                    
                    successful_uploads += len(data_to_insert)
                    print(f"‚úÖ {len(data_to_insert)} registros")
                    break
                    
                except Exception as retry_error:
                    if attempt == max_retries - 1:
                        raise retry_error
                    time.sleep(1)
            
            time.sleep(0.3)  # Pausa entre lotes
            
        except Exception as e:
            failed_uploads += len(chunk)
            error_msg = str(e)[:100]
            print(f"‚ùå {error_msg}...")
            
            # Se falhar nos primeiros lotes, para e reporta
            if chunk_index <= 3:
                print(f"üîç Erro cr√≠tico no lote {chunk_index}:")
                print(f"    Colunas: {len(chunk.columns)}")
                print(f"    Erro: {error_msg}")
                if chunk_index == 1:
                    print("‚ö†Ô∏è Parando no primeiro erro para an√°lise")
                    break
    
    # Relat√≥rio final
    print(f"\n{'='*60}")
    print(f"üìä RELAT√ìRIO FINAL:")
    print(f"  üì• Total processado: {len(df_clean):,}")
    print(f"  ‚úÖ Sucesso: {successful_uploads:,}")
    print(f"  ‚ùå Falhas: {failed_uploads:,}")
    print(f"  üìà Taxa de sucesso: {(successful_uploads/len(df_clean))*100:.1f}%")
    print(f"  üìã Colunas utilizadas: {len(available_columns)}")
    print(f"{'='*60}")
    
    return successful_uploads, failed_uploads, len(available_columns)

# --- 6. Execu√ß√£o principal ---
try:
    # Download e processamento
    df = download_and_process_data()
    
    if df.empty:
        print("‚ùå Nenhum dado processado.")
        sys.exit(1)
    
    print(f"‚úÖ Dados processados: {len(df):,} registros, {len(df.columns)} colunas")
    
    # Upload com auto-fix
    successful, failed, columns_used = upload_with_auto_fix(df)
    
    # Avalia√ß√£o final
    success_rate = (successful / len(df)) * 100 if len(df) > 0 else 0
    
    if success_rate >= 95:
        print("üéâ UPLOAD REALIZADO COM SUCESSO!")
        sys.exit(0)
    elif success_rate >= 80:
        print("‚ö†Ô∏è Upload parcialmente bem-sucedido")
        sys.exit(0)
    else:
        print("‚ùå Upload falhou - taxa de sucesso muito baixa")
        sys.exit(1)

except Exception as e:
    print(f"üí• Erro cr√≠tico: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
